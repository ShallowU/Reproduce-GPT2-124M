# 在本地终端执行
ssh -i <path-to-your-private-key> ubuntu@<lambda-instance-ip>

# 更新系统并安装必要工具
sudo apt update
sudo apt install git-lfs -y

# 设置 Hugging Face token 环境变量
export HF_TOKEN=""

# 克隆你的训练代码
git clone https://github.com/ShallowU/Reproduce-GPT2-124M.git
cd Reproduce-GPT2-124M

# 检查 Python 和 PyTorch 环境
python3 --version
python3 -c "import torch; print(f'PyTorch version: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print(f'GPU count: {torch.cuda.device_count()}')"

# 安装项目依赖
pip install huggingface_hub tiktoken matplotlib numpy transformers

# 验证安装
python3 -c "import huggingface_hub, tiktoken, matplotlib; print('All packages installed successfully')"

# 使用 token 登录
python3 -c "from huggingface_hub import login; login(token='')"




# 1. 检查CUDA库状态
find /usr -name "libcuda.so*" 2>/dev/null








# 1. 备份并移除32位库干扰
sudo mv /usr/lib/i386-linux-gnu/libcuda.so /usr/lib/i386-linux-gnu/libcuda.so.backup

# 2. 确保64位库链接正确
sudo ln -sf /usr/lib/x86_64-linux-gnu/libcuda.so.1 /usr/lib/x86_64-linux-gnu/libcuda.so

# 3. 在CUDA目录创建标准链接
sudo ln -sf /usr/lib/x86_64-linux-gnu/libcuda.so.1 /usr/local/cuda-12.4/lib64/libcuda.so

# 4. 更新库缓存
sudo ldconfig

# 5. 设置环境变量
export CUDA_HOME=/usr/local/cuda-12.4
export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib64:/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH








python train_gpt2_1A6000.py

tail -f log/log.txt

watch -n 1 nvidia-smi

python3 upload_hf_1A6000.py