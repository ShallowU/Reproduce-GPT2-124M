# Reproduce-GPT2-124M From Scratch

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

ğŸš€ **ä»é›¶å¼€å§‹å¤ç°GPT-2 (124M)æ¨¡å‹çš„å®Œæ•´è®­ç»ƒæµç¨‹**

è¿™ä¸ªé¡¹ç›®å®ç°äº†å®Œæ•´çš„GPT-2 124Må‚æ•°æ¨¡å‹çš„ä»å¤´è®­ç»ƒï¼ŒåŒ…æ‹¬åˆ†å¸ƒå¼è®­ç»ƒã€æ•°æ®å¤„ç†ã€æ¨¡å‹è¯„ä¼°å’Œå¯è§†åŒ–ç­‰åŠŸèƒ½ã€‚<br>
æ¨¡å‹huggingfaceåœ°å€ï¼š[GPT2-124M](https://huggingface.co/ShallowU/GPT2-124M)
![loss](./loss.png)

è®­ç»ƒç»“æœ(ç›¸æ¯”ç›®å‰æœ€æ–°æ¨¡å‹ï¼Œè¢«é™ç»´æ‰“å‡»ğŸ˜­)
```
Prompt: 'Hello, I'm a computer science student,'
Generated: > Hello, I'm a computer science student,so it's nice to be a computer geek. I'm not a big fan of the computer, so it's not going to be the most powerful and powerful computer in the world. 

```

```
Prompt: 'Hello, I'm a language model,'
Generated: > Hello, I'm a language model,and I'm always looking for people who are comfortable with it. So many of you know the same language on a regular basis, but most of you are not sure whether it is your native language or not. I'll explain that later.
```

## âœ¨ é¡¹ç›®ç‰¹è‰²

- ğŸ”¥ **å®Œæ•´çš„GPT-2æ¶æ„å®ç°** - ä»é›¶å®ç°Transformerã€æ³¨æ„åŠ›æœºåˆ¶ç­‰æ ¸å¿ƒç»„ä»¶
- âš¡ **é«˜æ•ˆåˆ†å¸ƒå¼è®­ç»ƒ** - æ”¯æŒå¤šGPUå¹¶è¡Œè®­ç»ƒï¼Œä¼˜åŒ–è®­ç»ƒé€Ÿåº¦
- ğŸ“Š **å®æ—¶ç›‘æ§** - è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–ï¼ŒæŸå¤±æ›²çº¿è‡ªåŠ¨ç»˜åˆ¶

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
Reproduce-GPT2-124M/
|-- pre_tokens
|   handle_tokens.ipynb      # ä½¿ç”¨colabé¢„å¤„ç†10B raw text
|   tranfer-tokens-hf.ipynb  # å°†é¢„å¤„ç†åçš„tokenså­˜åˆ°huaggingface
|-- TestWorkflow
|   train_gpt2_1A6000.py     # æµ‹è¯•ä¸‹è½½çš„æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
|   upload_hf_1A6000.py      # ä¸Šä¼ è®­ç»ƒç»“æœåˆ°Hugging Face
|-- generate.py              # æ–‡æœ¬ç”Ÿæˆè„šæœ¬
|-- hellaswag.py             # HellaSwagæ•°æ®é›†å¤„ç†è„šæœ¬(å¯é€‰ï¼Œæœ¬æ¬¡è®­ç»ƒå¹¶æ²¡æœ‰è¾¹è®­è¾¹ç”Ÿæˆæµ‹è¯•)
|-- requirements.txt         # ä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ train_gpt2_8A100.py      # ä¸»è®­ç»ƒè„šæœ¬ï¼ˆæ”¯æŒåˆ†å¸ƒå¼ï¼‰
|-- upload_hf_8A100.py       # ä¸Šä¼ è®­ç»ƒç»“æœåˆ°Hugging Face
â”œâ”€â”€ workflow.txt             # è¯¦ç»†çš„éƒ¨ç½²æµç¨‹

```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒé…ç½®

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/ShallowU/Reproduce-GPT2-124M.git
cd Reproduce-GPT2-124M

# å®‰è£…ä¾èµ–
# python >= 3.8, torch >= 2.0,cuda >= 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install huggingface_hub tiktoken matplotlib numpy transformers
```

### 2. æ•°æ®å‡†å¤‡

è®­ç»ƒæ—¶æ•°æ®é›†å°†è‡ªåŠ¨ä»Hugging Faceä¸‹è½½ï¼š
```bash
# æ•°æ®é›†ä¿¡æ¯
HuggingFace Repository: ShallowU/FineWeb-Edu-10B-Tokens-NPY
Size: 10B tokens (æ•™è‚²ç›¸å…³çš„é«˜è´¨é‡æ–‡æœ¬)
Format: NumPy arrays to efficiently load
```

### 3. å¼€å§‹è®­ç»ƒ

*è®¾ç½®ä½ çš„Hugging Face token*
```bash
# è®¾ç½®Hugging Face token
# è¿™å°†å…è®¸ä½ ä¸Šä¼ æ¨¡å‹å’Œæ•°æ®åˆ°Hugging Face
# è¯·åœ¨ https://huggingface.co/settings/tokens åˆ›å»ºä¸€ä¸ªtoken
# å¹¶å°†å…¶è®¾ç½®ä¸ºç¯å¢ƒå˜é‡
# æ›¿æ¢ "your_token_here" ä¸ºä½ çš„å®é™…token
export HF_TOKEN="your_token_here" 
```
#### å•GPUè®­ç»ƒ
```bash
python train_gpt2_8A100.py
```

#### å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ    
```bash

# 8å¡A100è®­ç»ƒç¤ºä¾‹
torchrun --standalone --nproc_per_node=8 train_gpt2_8A100.py
```
å…·ä½“è®­ç»ƒè¯¦ç»†ä¸€æ­¥æ­¥æµç¨‹è¯·å‚è€ƒ [workflow.txt](workflow.txt)

## ğŸ“Š è®­ç»ƒé…ç½®

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| æ¨¡å‹å‚æ•° | 124M | GPT-2 Smallé…ç½® |
| æ‰¹æ¬¡å¤§å° | 0.5 M tokens | é«˜æ•ˆçš„æ‰¹æ¬¡é…ç½® |
| å­¦ä¹ ç‡ | 6e-4 â†’ 6e-5 | ä½™å¼¦é€€ç«è°ƒåº¦ |
| è®­ç»ƒæ­¥æ•° | 19,073 | çº¦10B tokens |
| é¢„çƒ­æ­¥æ•° | 715 | å­¦ä¹ ç‡é¢„çƒ­ |
| åºåˆ—é•¿åº¦ | 1024 | æ ‡å‡†GPT-2é•¿åº¦ |

## ğŸ“ˆ è®­ç»ƒç»“æœ

### æ€§èƒ½æŒ‡æ ‡
- **è®­ç»ƒæ—¶é—´**: ~1å°æ—¶ (8x A100 80G), ~16å°æ—¶ (2x A100 40G)
- **éªŒè¯æŸå¤±**: ~3.0 (æ”¶æ•›)
- **ååé‡**: ~150K tokens/sec(8x A100 80G), ~20K tokens/sec(2x A100 40G)



## ğŸ¤ å‚è€ƒèµ„æ–™

- [Andrej Karpathy's nanoGPT](https://github.com/karpathy/nanoGPT)
- [Let's reproduce GPT-2](https://youtu.be/l8pRSuU81PU)
- [GPT-2 Paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [GPT-3 Paper](https://arxiv.org/abs/2005.14165)

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## ğŸ™ è‡´è°¢

æ„Ÿè°¢ Andrej Karpathy!!!

---

â­ **å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ªStaræ”¯æŒï¼**