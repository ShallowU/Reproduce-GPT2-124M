{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ca00f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first install the required packages in your Google Colab environment\n",
    "!pip install datasets tiktoken tqdm\n",
    "# then run the script below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedcb3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first install the required packages in your Google Colab environment\n",
    "# !pip install datasets tiktoken tqdm\n",
    "# then run the script below\n",
    "\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from google.colab import drive\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "# 1. Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. Define the path in your Google Drive to save the tokenized files.\n",
    "#    Make sure this folder exists in your Google Drive.\n",
    "drive_path = \"/content/drive/My Drive/FineWeb-Edu-Tokens\"\n",
    "os.makedirs(drive_path, exist_ok=True)\n",
    "\n",
    "remote_name = \"sample-10BT\"\n",
    "shard_size = int(1e8) # 100M tokens per shard\n",
    "\n",
    "# --- SCRIPT LOGIC ---\n",
    "\n",
    "# ✅ Use streaming=True to avoid downloading the entire dataset to the Colab disk.\n",
    "# This downloads and processes the data on-the-fly.\n",
    "print(\"Loading dataset in streaming mode...\")\n",
    "fw = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=remote_name, split=\"train\", streaming=True)\n",
    "\n",
    "# Init the tokenizer\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "eot = enc._special_tokens['<|endoftext|>'] # end of text token\n",
    "def tokenize(doc):\n",
    "    \"\"\"Tokenizes a single document and returns a numpy array of uint16 tokens.\"\"\"\n",
    "    tokens = [eot] # a special token to delimit documents\n",
    "    tokens.extend(enc.encode_ordinary(doc[\"text\"]))\n",
    "    tokens_np = np.array(tokens)\n",
    "    assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"Token dictionary too large for uint16\"\n",
    "    return tokens_np.astype(np.uint16)\n",
    "\n",
    "def write_datafile(filename, tokens_np):\n",
    "    \"\"\"Saves the token array to a .npy file.\"\"\"\n",
    "    np.save(filename, tokens_np)\n",
    "\n",
    "# Set the number of processes for parallel tokenization\n",
    "nprocs = max(1, os.cpu_count() // 2)\n",
    "print(f\"Using {nprocs} processes for tokenization...\")\n",
    "\n",
    "with mp.Pool(nprocs) as pool:\n",
    "    shard_index = 0\n",
    "    all_tokens_np = np.empty((shard_size,), dtype=np.uint16)\n",
    "    token_count = 0\n",
    "    progress_bar = None\n",
    "\n",
    "    # Use pool.imap for lazy iteration, which works well with streaming\n",
    "    for tokens in pool.imap(tokenize, fw, chunksize=16):\n",
    "        if token_count + len(tokens) < shard_size:\n",
    "            all_tokens_np[token_count : token_count + len(tokens)] = tokens\n",
    "            token_count += len(tokens)\n",
    "            if progress_bar is None:\n",
    "                progress_bar = tqdm(total=shard_size, unit=\"tokens\", desc=f\"Shard {shard_index}\")\n",
    "            progress_bar.update(len(tokens))\n",
    "        else:\n",
    "            split = \"val\" if shard_index == 0 else \"train\"\n",
    "            filename = os.path.join(drive_path, f\"edufineweb_{split}_{shard_index:06d}.npy\")\n",
    "\n",
    "            remainder = shard_size - token_count\n",
    "            if progress_bar is not None:\n",
    "                progress_bar.update(remainder)\n",
    "                progress_bar.close()\n",
    "\n",
    "            all_tokens_np[token_count : token_count + remainder] = tokens[:remainder]\n",
    "            print(f\"\\nWriting shard {shard_index} to {filename}...\")\n",
    "            write_datafile(filename, all_tokens_np)\n",
    "\n",
    "            shard_index += 1\n",
    "            progress_bar = None # Reset for the next shard\n",
    "\n",
    "            # Populate the next shard with the leftovers\n",
    "            leftover_tokens = tokens[remainder:]\n",
    "            all_tokens_np[0 : len(leftover_tokens)] = leftover_tokens\n",
    "            token_count = len(leftover_tokens)\n",
    "\n",
    "# Write the final shard\n",
    "if token_count != 0:\n",
    "    split = \"val\" if shard_index == 0 else \"train\"\n",
    "    filename = os.path.join(drive_path, f\"edufineweb_{split}_{shard_index:06d}.npy\")\n",
    "    print(f\"\\nWriting final shard {shard_index} to {filename}...\")\n",
    "    write_datafile(filename, all_tokens_np[:token_count])\n",
    "\n",
    "print(f\"\\n✅ Tokenization complete. Files are saved in your Google Drive folder: {drive_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
